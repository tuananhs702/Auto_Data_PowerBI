from googleapiclient.discovery import build
from google.oauth2 import service_account
import os
import io
import json
import pandas as pd
from googleapiclient.http import MediaIoBaseDownload

# ğŸ”¹ Láº¥y credentials tá»« GitHub Secrets
GDRIVE_CREDENTIALS = json.loads(os.getenv("GDRIVE_CREDENTIALS"))

# ğŸ”¹ Káº¿t ná»‘i vá»›i Google Drive API
SCOPES = ['https://www.googleapis.com/auth/drive']
creds = service_account.Credentials.from_service_account_info(GDRIVE_CREDENTIALS, scopes=SCOPES)
service = build('drive', 'v3', credentials=creds)

# ğŸ”¹ ID thÆ° má»¥c Google Drive cáº§n táº£i
FOLDER_ID = "1LyQOw0sTGUTGUxxmGZivAzB_aTBdlH6d"
SAVE_PATH = "downloads"

# ğŸ”¹ Táº¡o thÆ° má»¥c downloads náº¿u chÆ°a tá»“n táº¡i
os.makedirs(SAVE_PATH, exist_ok=True)

# ğŸ”¹ Láº¥y danh sÃ¡ch file trong thÆ° má»¥c Google Drive
query = f"'{FOLDER_ID}' in parents and trashed=false"
results = service.files().list(q=query, fields="files(id, name)").execute()
files = results.get('files', [])

if not files:
    print("âš ï¸ KhÃ´ng cÃ³ file nÃ o trong thÆ° má»¥c!")
else:
    for file in files:
        file_id = file['id']
        file_name = file['name']
        file_path = os.path.join(SAVE_PATH, file_name)

        # ğŸ”¹ Táº£i file tá»« Google Drive
        request = service.files().get_media(fileId=file_id)
        with open(file_path, "wb") as f:
            downloader = MediaIoBaseDownload(f, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()

        print(f"âœ… ÄÃ£ táº£i: {file_name}")

        # ğŸ”¹ Náº¿u file lÃ  Excel, chuyá»ƒn sang CSV
        if file_name.endswith(('.xls', '.xlsx')):
            csv_path = file_path.rsplit('.', 1)[0] + ".csv"
            df = pd.read_excel(file_path)
            df.to_csv(csv_path, index=False)
            os.remove(file_path)  # XÃ³a file gá»‘c Excel
            print(f"ğŸ”„ ÄÃ£ chuyá»ƒn {file_name} thÃ nh {os.path.basename(csv_path)}")

# ğŸ”¹ Cáº­p nháº­t summary_Br_daily.csv tá»« predictions_table_BR_daily.csv
summary_file = os.path.join(SAVE_PATH, "summary_Br_daily.csv")
predictions_file = os.path.join(SAVE_PATH, "predictions_table_BR_daily.csv")

if os.path.exists(summary_file) and os.path.exists(predictions_file):
    # ğŸ“Œ Äá»c dá»¯ liá»‡u
    df_summary = pd.read_csv(summary_file)
    df_predictions = pd.read_csv(predictions_file, header=None)

    # ğŸ“… Láº¥y danh sÃ¡ch ngÃ y má»›i tá»« file dá»± Ä‘oÃ¡n
    new_dates = df_predictions.iloc[:, 0].dropna().astype(str).tolist()
    new_data = df_predictions.iloc[:, 1:4].values  # Láº¥y dá»¯ liá»‡u 3 cá»™t tiáº¿p theo

    # Náº¿u file summary chÆ°a cÃ³ tÃªn cá»™t, Ä‘áº·t láº¡i tÃªn
    if df_summary.columns[0] != "Date":
        df_summary.rename(columns={df_summary.columns[0]: "Date"}, inplace=True)

    # ğŸŒŸ ThÃªm ngÃ y má»›i vÃ o summary náº¿u chÆ°a cÃ³
    existing_dates = df_summary["Date"].astype(str).tolist()  # Láº¥y cá»™t ngÃ y tá»« summary
    missing_data = [
        {"Date": date, "Predicted": new_data[i][0], "Upper_Bound": new_data[i][1], "Lower_Bound": new_data[i][2]}
        for i, date in enumerate(new_dates) if date not in existing_dates
    ]

    if missing_data:
        df_new = pd.DataFrame(missing_data)
        df_summary = pd.concat([df_summary, df_new], ignore_index=True)

    # ğŸ“ Äáº£m báº£o 3 cá»™t dá»¯ liá»‡u tá»“n táº¡i
    for col in ["Predicted", "Upper_Bound", "Lower_Bound"]:
        if col not in df_summary.columns:
            df_summary[col] = None  # Táº¡o cá»™t náº¿u chÆ°a cÃ³

    # ğŸ’¾ LÆ°u láº¡i file summary
    df_summary.to_csv(summary_file, index=False)
    print(f"âœ… ÄÃ£ cáº­p nháº­t {summary_file}")
else:
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y cáº£ 2 file summary_Br_daily.csv vÃ  predictions_table_BR_daily.csv!")
